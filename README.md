# OptimKit

**optimkit** is a lightweight Python toolkit for classical numerical optimization algorithms, with an emphasis on clarity, mathematical correctness, and educational value.
It provides clean NumPy- and SymPy-based implementations of **one-dimensional**, **multivariable**, and **population-based** optimization methods.

The project is designed to closely follow standard optimization theory while remaining practical and easy to extend.

---

## âœ¨ Features

* ğŸ“ **One-dimensional optimization**

  * Bisection
  * Derivative-based bisection
  * Fibonacci search
  * Golden-section search

* ğŸ“ˆ **Multivariable unconstrained optimization**

  * Steepest Descent
  * Newtonâ€™s Method
  * Levenbergâ€“Marquardt
  * Multiple step-size (Î³) selection strategies

* ğŸ§¬ **Metaheuristics / population-based methods**

  * Classical Genetic Algorithm (selection, crossover, mutation)
  * Grey Wolf Optimizer (GWO)

* ğŸ§  **Symbolic â†’ Numeric workflow**

  * Objective functions defined symbolically using **SymPy**
  * Automatically converted to fast numeric functions via **NumPy**

* ğŸ§© Modular, readable, and easy to extend

---

## âš ï¸ Problem Assumptions

At its current stage, **optimkit assumes unconstrained optimization problems**.

* No equality or inequality constraints are supported
* All multivariable methods operate on unconstrained objective functions
* Constraint handling (e.g. penalty methods, projections, Lagrange multipliers) is **not implemented yet**, but the project structure allows for future extensions.
  
Future versions may extend support to constrained optimization.

---

## ğŸ”§ Installation

Clone the repository:

```bash
git clone https://github.com/your-username/optimkit.git
cd optimkit
```

Install the dependencies:

```bash
pip install -r requirements.txt
```

### Requirements

* Python â‰¥ 3.9
* NumPy
* SymPy

---

## ğŸš€ Quick Usage Examples

### 1ï¸âƒ£ One-dimensional optimization

```python
from sympy import symbols
from optimkit.opt1d import bisection

x = symbols('x')
f = (x - 2)**2

low, high, ops = bisection(
    f=f,
    alpha=-5,
    beta=5,
    length_tol=1e-3,
    epsilon=1e-4
)

print("Approximate minimum:", (low[-1] + high[-1]) / 2)
```

---

### 2ï¸âƒ£ Multivariable optimization (Steepest Descent)

```python
import sympy as sp
import numpy as np
from optimkit.optNd import steepest_descent

x, y = sp.symbols('x y')
f = x**5 * sp.exp(-x**2 - y**2)

x_min, N, grad_norm, f_vals = steepest_descent(
    f=f,
    epsilon=1e-4,
    starting_point=np.array([-1.0, 1.0]),
    gamma_selection="constant",
    gamma=0.45,
    alpha=None,
    beta=None
)

print("Minimum found at:", x_min[-1])
```

---

### 3ï¸âƒ£ Step-size (Î³) selection

Supported strategies for multivariable methods:

* **Constant step size**
* **Armijo backtracking rule**
* **Optimal line search** (via line search on ( f(x_k + \gamma d_k) ))

---

## ğŸ“ Project Structure

```text
optimkit/
â”‚
â”œâ”€â”€ opt1d/              # One-dimensional optimization algorithms
â”‚   â”œâ”€â”€ bisection.py
â”‚   â”œâ”€â”€ diff_bisection.py
â”‚   â”œâ”€â”€ fibonacci.py
â”‚   â””â”€â”€ golden_sector.py
â”‚
â”œâ”€â”€ optNd/              # Multivariable optimization algorithms
â”‚   â”œâ”€â”€ steepest_descent.py
â”‚   â”œâ”€â”€ newton.py
â”‚   â”œâ”€â”€ levenberg_marquardt.py
â”‚   â””â”€â”€ helper_utils.py
â”‚
â”œâ”€â”€ genetic_opt/        # Population-based optimization methods
â”‚   â”œâ”€â”€ classicGA/
â”‚   â”‚   â”œâ”€â”€ GA.py
â”‚   â”‚   â”œâ”€â”€ selection.py
â”‚   â”‚   â”œâ”€â”€ crossover.py
â”‚   â”‚   â””â”€â”€ mutation.py
â”‚   â””â”€â”€ GWO/
â”‚       â””â”€â”€ GWO.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  Design Philosophy

* Prefer **clarity over excessive abstraction**
* Algorithms closely follow textbook formulations
* Symbolic definitions first, numeric execution second
* Minimal dependencies
* Suitable for:

  * academic projects
  * numerical optimization coursework
  * research prototypes

---

## ğŸ›£ï¸ Roadmap

Planned improvements:

* Constraint handling techniques
* Quasi-Newton methods (BFGS, L-BFGS)
* Additional metaheuristics
* Documentation generation (Sphinx)

---

## ğŸ“œ License

This project is released under the **MIT License**.

You are free to use, modify, and distribute it with attribution.

---

## ğŸ“š Citation (optional)

If you use **optimkit** in academic work, consider citing it as a software artifact.
